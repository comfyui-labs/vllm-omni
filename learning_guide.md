# vLLM-Omni ä»£ç æ·±åº¦åˆ†æä¸å­¦ä¹ æŒ‡å—

> æœ¬æ–‡æ¡£ä¸º vLLM-Omni é¡¹ç›®çš„ä»£ç åˆ†æå’Œå°ç™½å­¦ä¹ è·¯çº¿æŒ‡å—ã€‚

## ç›®å½•

- [é¡¹ç›®å®šä½](#é¡¹ç›®å®šä½)
- [æ ¸å¿ƒæ¶æ„æ¦‚è§ˆ](#æ ¸å¿ƒæ¶æ„æ¦‚è§ˆ)
- [äº”å¤§æ ¸å¿ƒæ¨¡å—è¯¦è§£](#äº”å¤§æ ¸å¿ƒæ¨¡å—è¯¦è§£)
  - [å…¥å£ç‚¹æ¨¡å—](#1ï¸âƒ£-å…¥å£ç‚¹æ¨¡å—-entrypoints)
  - [Diffusion æ¨¡å—](#2ï¸âƒ£-diffusion-æ¨¡å—)
  - [AR æ¨¡å—](#3ï¸âƒ£-ar-autoregressive-æ¨¡å—)
  - [åˆ†å¸ƒå¼è¿æ¥å™¨](#4ï¸âƒ£-åˆ†å¸ƒå¼è¿æ¥å™¨-omniconnector)
  - [é…ç½®ç³»ç»Ÿ](#5ï¸âƒ£-é…ç½®ç³»ç»Ÿ)
- [å°ç™½å­¦ä¹ è·¯çº¿å»ºè®®](#å°ç™½å­¦ä¹ è·¯çº¿å»ºè®®)
- [é¡¹ç›®äº®ç‚¹æ€»ç»“](#é¡¹ç›®äº®ç‚¹æ€»ç»“)

---

## é¡¹ç›®å®šä½

**vLLM-Omni** æ˜¯ vLLM çš„æ‰©å±•æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºæ”¯æŒå…¨æ¨¡æ€ï¼ˆOmni-modalityï¼‰æ¨¡å‹çš„æ¨ç†å’ŒæœåŠ¡ã€‚å®ƒå°† vLLM ä»ä¼ ç»Ÿçš„æ–‡æœ¬è‡ªå›å½’ç”Ÿæˆæ‰©å±•åˆ°æ”¯æŒï¼š

- **å¤šæ¨¡æ€è¾“å…¥/è¾“å‡º**ï¼šæ–‡æœ¬ã€å›¾åƒã€è§†é¢‘ã€éŸ³é¢‘
- **éè‡ªå›å½’æ¶æ„**ï¼šDiffusion Transformer (DiT) ç­‰å¹¶è¡Œç”Ÿæˆæ¨¡å‹
- **å¼‚æ„è¾“å‡º**ï¼šä»ä¼ ç»Ÿæ–‡æœ¬åˆ°å¤šæ¨¡æ€è¾“å‡º

### æ”¯æŒçš„æ¨¡å‹ç±»å‹

æ ¹æ®å½“å‰æµè¡Œå¼€æºæ¨¡å‹çš„åˆ†æï¼Œå¤§å¤šæ•°å…¨æ¨¡æ€æ¨¡å‹éƒ½æ˜¯ AR + DiT çš„ç»„åˆï¼š

| ç±»å‹ | ç¤ºä¾‹ | æè¿° |
|------|------|------|
| **DiT ä¸ºä¸»ï¼ŒAR ä¸ºæ–‡æœ¬ç¼–ç å™¨** | Qwen-Image | å¼ºå¤§çš„å›¾åƒç”ŸæˆåŸºç¡€æ¨¡å‹ |
| **AR ä¸ºä¸»ï¼ŒDiT ä¸ºå¤šæ¨¡æ€ç”Ÿæˆå™¨** | BAGEL | ç»Ÿä¸€çš„å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæ¨¡å‹ |
| **AR + DiT æ··åˆ** | Qwen-Omni | ç«¯åˆ°ç«¯å…¨æ¨¡æ€ LLM |

---

## æ ¸å¿ƒæ¶æ„æ¦‚è§ˆ

### ç›®å½•ç»“æ„

```
vllm-omni/
â”œâ”€â”€ vllm_omni/                    # æ ¸å¿ƒæºç 
â”‚   â”œâ”€â”€ __init__.py              # åŒ…å…¥å£
â”‚   â”œâ”€â”€ config/                   # é…ç½®æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ lora.py              # LoRA é…ç½®
â”‚   â”‚   â””â”€â”€ model.py             # OmniModelConfig
â”‚   â”œâ”€â”€ core/                     # è°ƒåº¦å™¨æ ¸å¿ƒ
â”‚   â”‚   â””â”€â”€ sched/               # è°ƒåº¦å™¨å®ç°
â”‚   â”œâ”€â”€ diffusion/                # Diffusion æ¨¡å— (æ ¸å¿ƒ!)
â”‚   â”‚   â”œâ”€â”€ attention/           # æ³¨æ„åŠ›æœºåˆ¶
â”‚   â”‚   â”œâ”€â”€ cache/               # ç¼“å­˜åŠ é€Ÿ
â”‚   â”‚   â”œâ”€â”€ distributed/         # åˆ†å¸ƒå¼
â”‚   â”‚   â”œâ”€â”€ models/              # æ¨¡å‹å®ç°
â”‚   â”‚   â”œâ”€â”€ worker/              # Worker
â”‚   â”‚   â”œâ”€â”€ diffusion_engine.py  # ä¸»å¼•æ“
â”‚   â”‚   â””â”€â”€ scheduler.py         # è°ƒåº¦å™¨
â”‚   â”œâ”€â”€ distributed/              # åˆ†å¸ƒå¼é€šä¿¡
â”‚   â”‚   â”œâ”€â”€ omni_connectors/     # è¿æ¥å™¨å®ç°
â”‚   â”‚   â””â”€â”€ ray_utils/           # Ray å·¥å…·
â”‚   â”œâ”€â”€ engine/                   # å¼•æ“å±‚
â”‚   â”‚   â”œâ”€â”€ arg_utils.py
â”‚   â”‚   â”œâ”€â”€ input_processor.py
â”‚   â”‚   â””â”€â”€ output_processor.py
â”‚   â”œâ”€â”€ entrypoints/              # å…¥å£ç‚¹ (API å±‚)
â”‚   â”‚   â”œâ”€â”€ omni.py              # ä¸»å…¥å£ Omni ç±»
â”‚   â”‚   â”œâ”€â”€ async_omni.py        # å¼‚æ­¥å…¥å£
â”‚   â”‚   â”œâ”€â”€ cli/                 # å‘½ä»¤è¡Œå·¥å…·
â”‚   â”‚   â””â”€â”€ openai/              # OpenAI å…¼å®¹ API
â”‚   â”œâ”€â”€ inputs/                   # è¾“å…¥å¤„ç†
â”‚   â”‚   â”œâ”€â”€ data.py              # æ•°æ®ç±»å‹å®šä¹‰
â”‚   â”‚   â”œâ”€â”€ parse.py             # è§£æ
â”‚   â”‚   â””â”€â”€ preprocess.py        # é¢„å¤„ç†
â”‚   â”œâ”€â”€ model_executor/           # æ¨¡å‹æ‰§è¡Œå™¨
â”‚   â”‚   â”œâ”€â”€ models/              # æ¨¡å‹å®ç°
â”‚   â”‚   â””â”€â”€ stage_configs/       # é˜¶æ®µé…ç½® YAML
â”‚   â”œâ”€â”€ platforms/                # å¤šå¹³å°æ”¯æŒ
â”‚   â”‚   â”œâ”€â”€ cuda/                # CUDA
â”‚   â”‚   â”œâ”€â”€ npu/                 # NPU (åä¸º)
â”‚   â”‚   â”œâ”€â”€ rocm/                # ROCm (AMD)
â”‚   â”‚   â””â”€â”€ xpu/                 # XPU (Intel)
â”‚   â”œâ”€â”€ worker/                   # Worker å®ç°
â”‚   â”œâ”€â”€ outputs.py               # è¾“å‡ºæ•°æ®ç»“æ„
â”‚   â””â”€â”€ request.py               # è¯·æ±‚æ•°æ®ç»“æ„
â”œâ”€â”€ examples/                     # ç¤ºä¾‹ä»£ç 
â”‚   â”œâ”€â”€ offline_inference/       # ç¦»çº¿æ¨ç†ç¤ºä¾‹
â”‚   â””â”€â”€ online_serving/          # åœ¨çº¿æœåŠ¡ç¤ºä¾‹
â”œâ”€â”€ docs/                         # è®¾è®¡æ–‡æ¡£
â”‚   â”œâ”€â”€ design/                  # è®¾è®¡æ–‡æ¡£
â”‚   â”œâ”€â”€ getting_started/         # å…¥é—¨æŒ‡å—
â”‚   â””â”€â”€ user_guide/              # ç”¨æˆ·æŒ‡å—
â”œâ”€â”€ tests/                        # æµ‹è¯•ç”¨ä¾‹
â”œâ”€â”€ benchmarks/                   # åŸºå‡†æµ‹è¯•
â””â”€â”€ pyproject.toml               # é¡¹ç›®é…ç½®
```

### ä¸»è¦ç»„ä»¶å…³ç³»å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ç”¨æˆ·è¯·æ±‚                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Omni / AsyncOmni                             â”‚
â”‚                    (entrypoints/omni.py)                        â”‚
â”‚              ç»Ÿä¸€å…¥å£ï¼Œç®¡é“ç¼–æ’ï¼Œè¯·æ±‚è°ƒåº¦                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      OmniStage (å¤šä¸ª)                            â”‚
â”‚              æ¯ä¸ªé˜¶æ®µå¯ä»¥æ˜¯ AR æˆ– Diffusion                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚   Stage 0 (AR)  â”‚â”€â”€â”€â–¶â”‚  Stage 1 (AR)   â”‚â”€â”€â”€â–¶â”‚Stage 2 (Conv) â”‚ â”‚
â”‚ â”‚    Thinker      â”‚    â”‚     Talker      â”‚    â”‚   Code2wav    â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                      â”‚                      â”‚         â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                    OmniConnector (æ•°æ®ä¼ è¾“)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    OmniRequestOutput                            â”‚
â”‚              åŒ…å«æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ç­‰å¤šæ¨¡æ€è¾“å‡º                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## äº”å¤§æ ¸å¿ƒæ¨¡å—è¯¦è§£

### 1ï¸âƒ£ å…¥å£ç‚¹æ¨¡å— (entrypoints)

**ä½ç½®**: `vllm_omni/entrypoints/`

**æ ¸å¿ƒç±»**:

```python
# vllm_omni/entrypoints/omni.py

class OmniBase:
    """Base class for serving Omni models.

    Args:
        model: Model name or path to load.
        **kwargs: Arbitrary keyword arguments.
            - stage_configs_path: é˜¶æ®µé…ç½® YAML è·¯å¾„
            - log_stats: æ˜¯å¦å¯ç”¨ç»Ÿè®¡æ—¥å¿—
            - stage_init_timeout: é˜¶æ®µåˆå§‹åŒ–è¶…æ—¶æ—¶é—´
            - shm_threshold_bytes: å…±äº«å†…å­˜é˜ˆå€¼
            - worker_backend: Worker åç«¯ ("multi_process" æˆ– "ray")
            - ray_address: Ray é›†ç¾¤åœ°å€
            - batch_timeout: æ‰¹å¤„ç†è¶…æ—¶æ—¶é—´
            - init_timeout: åˆå§‹åŒ–è¶…æ—¶æ—¶é—´
    """

class Omni(OmniBase):
    """ç»Ÿä¸€å…¥å£ï¼Œæ”¯æŒ LLM å’Œ Diffusion æ¨¡å‹"""
    
    def generate(
        self,
        prompts: OmniPromptType | Sequence[OmniPromptType],
        sampling_params_list: OmniSamplingParams | Sequence[OmniSamplingParams] | None = None,
        *,
        py_generator: bool = False,
        use_tqdm: bool | Callable[..., tqdm] = True,
    ) -> Generator[OmniRequestOutput, None, None] | list[OmniRequestOutput]:
        """ç”Ÿæˆè¾“å‡º"""
        ...
```

**æ ¸å¿ƒèŒè´£**:
- ç»Ÿä¸€çš„æ¨ç†å…¥å£ï¼ˆç¦»çº¿æ‰¹é‡æ¨ç†å’Œåœ¨çº¿æœåŠ¡ï¼‰
- å¤šé˜¶æ®µç®¡é“ç¼–æ’ï¼ˆå¦‚ Thinker â†’ Talker â†’ Code2wavï¼‰
- è¯·æ±‚è°ƒåº¦å’Œç»“æœæ”¶é›†
- èµ„æºç®¡ç†å’Œæ¸…ç†

**å…³é”®æ–¹æ³•**:

| æ–¹æ³• | æè¿° |
|------|------|
| `__init__()` | åˆå§‹åŒ–æ¨¡å‹å’Œé˜¶æ®µ |
| `generate()` | æ‰§è¡Œæ¨ç†ç”Ÿæˆ |
| `_initialize_stages()` | åˆå§‹åŒ–æ‰€æœ‰é˜¶æ®µ |
| `_start_stages()` | å¯åŠ¨æ‰€æœ‰é˜¶æ®µè¿›ç¨‹ |
| `start_profile()` / `stop_profile()` | æ€§èƒ½åˆ†æ |
| `close()` | æ¸…ç†èµ„æº |

---

### 2ï¸âƒ£ Diffusion æ¨¡å—

**ä½ç½®**: `vllm_omni/diffusion/`

è¿™æ˜¯ vLLM-Omni æœ€æ ¸å¿ƒçš„åˆ›æ–°æ¨¡å—ï¼Œå®ç°äº†éè‡ªå›å½’çš„ Diffusion æ¨ç†ã€‚

#### ç›®å½•ç»“æ„

```
vllm_omni/diffusion/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ attention/                    # æ³¨æ„åŠ›æœºåˆ¶
â”‚   â”œâ”€â”€ backends/                # åç«¯å®ç°
â”‚   â”‚   â”œâ”€â”€ abstract.py         # æŠ½è±¡åŸºç±»
â”‚   â”‚   â”œâ”€â”€ flash_attn.py       # FlashAttention
â”‚   â”‚   â”œâ”€â”€ sdpa.py             # PyTorch SDPA
â”‚   â”‚   â”œâ”€â”€ sage_attn.py        # SageAttention
â”‚   â”‚   â””â”€â”€ registry.py         # åç«¯æ³¨å†Œ
â”‚   â”œâ”€â”€ parallel/                # å¹¶è¡Œæ³¨æ„åŠ›
â”‚   â”‚   â”œâ”€â”€ ring.py             # Ring Attention
â”‚   â”‚   â””â”€â”€ ulysses.py          # Ulysses SP
â”‚   â”œâ”€â”€ layer.py                 # Attention å±‚
â”‚   â””â”€â”€ selector.py              # åç«¯é€‰æ‹©å™¨
â”œâ”€â”€ cache/                        # ç¼“å­˜åŠ é€Ÿ
â”‚   â”œâ”€â”€ base.py                  # ç¼“å­˜åŸºç±»
â”‚   â”œâ”€â”€ cache_dit_backend.py    # cache-dit
â”‚   â”œâ”€â”€ teacache/                # TeaCache
â”‚   â””â”€â”€ selector.py              # ç¼“å­˜é€‰æ‹©å™¨
â”œâ”€â”€ distributed/                  # åˆ†å¸ƒå¼
â”‚   â”œâ”€â”€ parallel_state.py       # å¹¶è¡ŒçŠ¶æ€ç®¡ç†
â”‚   â”œâ”€â”€ cfg_parallel.py         # CFG å¹¶è¡Œ
â”‚   â”œâ”€â”€ sp_plan.py              # åºåˆ—å¹¶è¡Œè®¡åˆ’
â”‚   â””â”€â”€ comm.py                  # é€šä¿¡
â”œâ”€â”€ models/                       # æ¨¡å‹å®ç°
â”‚   â”œâ”€â”€ interface.py            # æ¨¡å‹æ¥å£
â”‚   â”œâ”€â”€ qwen_image/             # Qwen-Image
â”‚   â”œâ”€â”€ flux/                   # FLUX
â”‚   â”œâ”€â”€ flux2_klein/            # FLUX2-Klein
â”‚   â”œâ”€â”€ glm_image/              # GLM-Image
â”‚   â”œâ”€â”€ wan2_2/                 # Wan2.2
â”‚   â”œâ”€â”€ z_image/                # Z-Image
â”‚   â”œâ”€â”€ bagel/                  # BAGEL
â”‚   â”œâ”€â”€ stable_audio/           # Stable Audio
â”‚   â””â”€â”€ schedulers/             # è°ƒåº¦å™¨
â”œâ”€â”€ executor/                     # æ‰§è¡Œå™¨
â”‚   â”œâ”€â”€ abstract.py
â”‚   â””â”€â”€ multiproc_executor.py
â”œâ”€â”€ worker/                       # Worker
â”‚   â”œâ”€â”€ diffusion_worker.py
â”‚   â””â”€â”€ diffusion_model_runner.py
â”œâ”€â”€ layers/                       # è‡ªå®šä¹‰å±‚
â”‚   â”œâ”€â”€ adalayernorm.py
â”‚   â”œâ”€â”€ rope.py
â”‚   â””â”€â”€ custom_op.py
â”œâ”€â”€ lora/                         # LoRA æ”¯æŒ
â”œâ”€â”€ hooks/                        # é’©å­ç³»ç»Ÿ
â”œâ”€â”€ profiler/                     # æ€§èƒ½åˆ†æ
â”œâ”€â”€ model_loader/                 # æ¨¡å‹åŠ è½½
â”œâ”€â”€ diffusion_engine.py           # ä¸»å¼•æ“
â”œâ”€â”€ scheduler.py                  # è°ƒåº¦å™¨
â”œâ”€â”€ request.py                    # è¯·æ±‚å®šä¹‰
â”œâ”€â”€ data.py                       # æ•°æ®ç»“æ„
â”œâ”€â”€ compile.py                    # ç¼–è¯‘ä¼˜åŒ–
â”œâ”€â”€ forward_context.py            # å‰å‘ä¸Šä¸‹æ–‡
â”œâ”€â”€ offload.py                    # å†…å­˜å¸è½½
â”œâ”€â”€ envs.py                       # ç¯å¢ƒå˜é‡
â”œâ”€â”€ registry.py                   # æ¨¡å‹æ³¨å†Œ
â””â”€â”€ utils/                        # å·¥å…·å‡½æ•°
```

#### æ ¸å¿ƒç»„ä»¶

**1. DiffusionEngine (ä¸»å¼•æ“)**

```python
# vllm_omni/diffusion/diffusion_engine.py

class DiffusionEngine:
    """Diffusion æ¨ç†å¼•æ“ï¼Œç®¡ç† Worker è¿›ç¨‹å’Œæ‰§è¡Œæµç¨‹"""
    
    def __init__(self, od_config: OmniDiffusionConfig):
        self.od_config = od_config
        self.post_process_func = get_diffusion_post_process_func(od_config)
        self.pre_process_func = get_diffusion_pre_process_func(od_config)
        self._processes: list[mp.Process] = []
        self._make_client()
    
    def step(self, requests: list[OmniDiffusionRequest]):
        """æ‰§è¡Œä¸€æ­¥æ¨ç†"""
        # 1. é¢„å¤„ç†è¯·æ±‚
        requests = self.pre_process_func(requests)
        # 2. å‘é€åˆ°è°ƒåº¦å™¨å¹¶ç­‰å¾…å“åº”
        output = self.add_req_and_wait_for_response(requests)
        # 3. åå¤„ç†ç»“æœ
        result = self.post_process_func(output.output)
        return result
```

**2. Scheduler (è°ƒåº¦å™¨)**

```python
# vllm_omni/diffusion/scheduler.py

class Scheduler:
    """å•ä¾‹è°ƒåº¦å™¨ï¼Œåè°ƒæ‰€æœ‰ Worker"""
    
    _instance = None
    
    def __new__(cls, *args, **kwargs):
        if not cls._instance:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def add_req(self, requests: list[OmniDiffusionRequest]) -> DiffusionOutput:
        """å¹¿æ’­è¯·æ±‚åˆ°æ‰€æœ‰ Worker"""
        self.mq.enqueue(requests)
        output = self.result_mq.dequeue()
        return output
```

**3. æ³¨æ„åŠ›åç«¯**

æ”¯æŒå¤šç§æ³¨æ„åŠ›å®ç°ï¼š

| åç«¯ | æè¿° | é€‚ç”¨åœºæ™¯ |
|------|------|----------|
| `FlashAttention` | é«˜æ€§èƒ½ CUDA å†…æ ¸ | NVIDIA GPU |
| `SDPA` | PyTorch å†…ç½® | è·¨å¹³å°é»˜è®¤ |
| `SageAttention` | ç¨€ç–æ³¨æ„åŠ› | é•¿åºåˆ— |
| `AscendAttention` | NPU ä¼˜åŒ– | åä¸ºæ˜‡è…¾ |

**4. å¹¶è¡Œç­–ç•¥**

```python
# åˆå§‹åŒ–å¹¶è¡Œç»„
def initialize_model_parallel(
    data_parallel_size: int = 1,      # æ•°æ®å¹¶è¡Œ
    cfg_parallel_size: int = 1,       # CFG å¹¶è¡Œ
    sequence_parallel_size: int = 1,  # åºåˆ—å¹¶è¡Œ (ulysses_degree Ã— ring_degree)
    tensor_parallel_size: int = 1,    # å¼ é‡å¹¶è¡Œ
    pipeline_parallel_size: int = 1,  # æµæ°´çº¿å¹¶è¡Œ
):
    ...
```

**5. ç¼“å­˜åŠ é€Ÿ**

| åç«¯ | ç‰¹æ€§ |
|------|------|
| `TeaCache` | åŸºäºæ—¶é—´æ­¥åµŒå…¥ç›¸ä¼¼åº¦çš„ç¼“å­˜ |
| `cache-dit` | DBCache + SCM + TaylorSeer |

---

### 3ï¸âƒ£ AR (AutoRegressive) æ¨¡å—

**ä½ç½®**: åˆ†å¸ƒåœ¨ `vllm_omni/core/`, `vllm_omni/worker/`, `vllm_omni/model_executor/`

AR æ¨¡å—é€šè¿‡ç»§æ‰¿æ‰©å±• vLLM çš„æ ¸å¿ƒç»„ä»¶ï¼š

#### ç»§æ‰¿å±‚æ¬¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Scheduler                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM Scheduler â”€â”€â–¶ OmniARScheduler                          â”‚
â”‚                 â””â”€â–¶ OmniGenerationScheduler                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Worker                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPUWorker â”€â”€â–¶ GPUARWorker                                   â”‚
â”‚           â””â”€â–¶ GPUGenerationWorker                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       ModelRunner                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPUModelRunner â”€â”€â–¶ OmniGPUModelRunner â”€â”€â–¶ GPUARModelRunner  â”‚
â”‚                                       â””â”€â–¶ GPUGenerationModelRunner â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Input/Output Processor                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ InputProcessor â”€â”€â–¶ OmniInputProcessor                       â”‚
â”‚ OutputProcessor â”€â”€â–¶ MultimodalOutputProcessor               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### ä¸»è¦æ‰©å±•åŠŸèƒ½

| åŠŸèƒ½ | æè¿° |
|------|------|
| **Payload æ”¯æŒ** | åºåˆ—åŒ– prompt embeddings å’Œé™„åŠ ä¿¡æ¯ï¼Œæ”¯æŒé˜¶æ®µé—´ä¼ é€’ |
| **å¤šæ¨¡æ€å¤„ç†** | æ‰©å±•çš„è¾“å…¥/è¾“å‡ºå¤„ç†å™¨æ”¯æŒå›¾åƒã€éŸ³é¢‘ç­‰ |
| **Hidden State æš´éœ²** | AR æ¨¡å‹è¿è¡Œå™¨é€šè¿‡ `pooler_output` æš´éœ² hidden states |
| **ç”Ÿæˆè°ƒåº¦å™¨** | é’ˆå¯¹éè‡ªå›å½’æ¶æ„çš„å¿«é€Ÿè°ƒåº¦è·¯å¾„ |

#### è¯·æ±‚æµç¨‹

```
OmniInputProcessor
       â”‚
       â–¼ OmniEngineCoreRequest (å¸¦ payload)
OmniARScheduler
       â”‚
       â–¼ schedule: OmniNewRequestData
GPUARWorker
       â”‚
       â–¼ SchedulerOutput
GPUARModelRunner
       â”‚
       â–¼ execute_model
Model Forward Pass
       â”‚
       â–¼ hidden_states, logits
GPUARModelRunner
       â”‚
       â–¼ sample_tokens: OmniModelRunnerOutput
OmniARScheduler
       â”‚
       â–¼ update_from_output
MultimodalOutputProcessor
       â”‚
       â–¼ RequestOutput
Client/Downstream Stage
```

---

### 4ï¸âƒ£ åˆ†å¸ƒå¼è¿æ¥å™¨ (OmniConnector)

**ä½ç½®**: `vllm_omni/distributed/omni_connectors/`

å®ç°é˜¶æ®µé—´çš„æ•°æ®ä¼ è¾“ã€‚

#### æ ¸å¿ƒæ¥å£

```python
class OmniConnectorBase(ABC):
    @abstractmethod
    def put(
        self, 
        from_stage: str, 
        to_stage: str, 
        put_key: str, 
        data: Any
    ) -> tuple[bool, int, Optional[dict]]:
        """
        å­˜å‚¨æ•°æ®
        Returns: (success, serialized_size, metadata)
        """
        pass

    @abstractmethod
    def get(
        self, 
        from_stage: str, 
        to_stage: str, 
        get_key: str, 
        metadata: Optional[dict] = None
    ) -> Optional[tuple[Any, int]]:
        """
        è·å–æ•°æ®
        Returns: (object, serialized_size)
        """
        pass
```

#### æ”¯æŒçš„è¿æ¥å™¨

| è¿æ¥å™¨ | ä½¿ç”¨åœºæ™¯ | è¯´æ˜ |
|--------|----------|------|
| `SharedMemoryConnector` | å•èŠ‚ç‚¹ | é»˜è®¤ï¼Œæ— éœ€é…ç½® |
| `MooncakeConnector` | å¤šèŠ‚ç‚¹ | éœ€è¦ Mooncake Master |
| `YuanrongConnector` | å¤šèŠ‚ç‚¹ | éœ€è¦ Yuanrong Datasystem |

#### é…ç½®ç¤ºä¾‹

```yaml
runtime:
  connectors:
    connector_of_shared_memory:
      name: SharedMemoryConnector
      extra:
        shm_threshold_bytes: 65536

stage_args:
  - stage_id: 0
    output_connectors:
      to_stage_1: connector_of_shared_memory

  - stage_id: 1
    input_connectors:
      from_stage_0: connector_of_shared_memory
```

---

### 5ï¸âƒ£ é…ç½®ç³»ç»Ÿ

**ä½ç½®**: `vllm_omni/config/`, `vllm_omni/model_executor/stage_configs/`

#### OmniModelConfig

```python
# vllm_omni/config/model.py

@dataclass
class OmniModelConfig(ModelConfig):
    """Omni æ¨¡å‹é…ç½®ï¼Œæ‰©å±• vLLM ModelConfig"""
    
    stage_id: int = 0                    # é˜¶æ®µ ID
    async_chunk: bool = False            # å¼‚æ­¥åˆ†å—
    model_stage: str = "thinker"         # é˜¶æ®µç±»å‹
    model_arch: str = "Qwen2_5OmniForConditionalGeneration"  # æ¶æ„
    engine_output_type: str | None = None  # è¾“å‡ºç±»å‹
    hf_config_name: str | None = None    # HF é…ç½®å
    stage_connector_config: dict = field(
        default_factory=lambda: {
            "name": "SharedMemoryConnector",
            "extra": {},
        }
    )
    omni_kv_config: dict | None = None   # KV é…ç½®
```

#### é˜¶æ®µé…ç½® YAML ç¤ºä¾‹

```yaml
# Qwen3-Omni ä¸‰é˜¶æ®µé…ç½®ç¤ºä¾‹
stage_args:
  - stage_id: 0
    stage_type: "ar"
    final_output: true
    final_output_type: "text"
    runtime:
      process: true
      devices: "0"
      max_batch_size: 1
    engine_args:
      model_stage: "thinker"
      model_arch: "Qwen3OmniThinkerForConditionalGeneration"
      hf_config_name: "thinker_config"
      max_model_len: 32768
      trust_remote_code: true
      limit_mm_per_prompt:
        image: 1
        video: 1
        audio: 1

  - stage_id: 1
    stage_type: "ar"
    runtime:
      process: true
      devices: "0"
    engine_args:
      model_stage: "talker"
      model_arch: "Qwen3OmniTalkerForConditionalGeneration"
      hf_config_name: "talker_config"

  - stage_id: 2
    stage_type: "generation"
    final_output: true
    final_output_type: "audio"
    runtime:
      process: true
      devices: "0"
    engine_args:
      model_stage: "code2wav"
      model_arch: "Qwen3OmniCode2WavForConditionalGeneration"
```

---

## å°ç™½å­¦ä¹ è·¯çº¿å»ºè®®

### ğŸ“– é˜¶æ®µä¸€ï¼šåŸºç¡€å‡†å¤‡ï¼ˆ1-2 å‘¨ï¼‰

**å­¦ä¹ ç›®æ ‡**ï¼šç†è§£ vLLM å’Œå¤§æ¨¡å‹æ¨ç†åŸºç¡€

#### 1. å…ˆå­¦ä¹  vLLM åŸºç¡€

```bash
# é˜…è¯» vLLM å®˜æ–¹æ–‡æ¡£
https://docs.vllm.ai
```

**é‡ç‚¹ç†è§£**ï¼š
- PagedAttention å’Œ KV Cache æœºåˆ¶
- vLLM çš„æ¶æ„è®¾è®¡ï¼ˆSchedulerã€Workerã€ModelRunnerï¼‰
- è¯·æ±‚è°ƒåº¦å’Œæ‰¹å¤„ç†

#### 2. ç†è§£ Transformer å’Œ Diffusion

**Transformer**:
- è‡ªæ³¨æ„åŠ›æœºåˆ¶
- ç¼–ç å™¨-è§£ç å™¨æ¶æ„
- è‡ªå›å½’ç”Ÿæˆ

**Diffusion**:
- DDPM (Denoising Diffusion Probabilistic Models)
- Score Matching
- DiT (Diffusion Transformer) æ¶æ„

#### 3. å»ºè®®é˜…è¯»èµ„æ–™

| èµ„æ–™ | æè¿° |
|------|------|
| ã€ŠAttention Is All You Needã€‹ | Transformer åŸå§‹è®ºæ–‡ |
| ã€ŠDenoising Diffusion Probabilistic Modelsã€‹ | DDPM è®ºæ–‡ |
| ã€ŠScalable Diffusion Models with Transformersã€‹ | DiT è®ºæ–‡ |
| vLLM æŠ€æœ¯åšå®¢ | ç†è§£ PagedAttention |

---

### ğŸ“– é˜¶æ®µäºŒï¼šå¿«é€Ÿä¸Šæ‰‹ï¼ˆ1 å‘¨ï¼‰

**å­¦ä¹ ç›®æ ‡**ï¼šèƒ½å¤Ÿè¿è¡Œç¤ºä¾‹ä»£ç 

#### 1. å®‰è£…ç¯å¢ƒ

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
uv venv --python 3.12 --seed
source .venv/bin/activate

# å®‰è£… vLLM
uv pip install vllm==0.14.0 --torch-backend=auto

# å…‹éš†å¹¶å®‰è£… vLLM-Omni
git clone https://github.com/vllm-project/vllm-omni.git
cd vllm-omni
uv pip install -e .
```

#### 2. è¿è¡Œæœ€ç®€å•çš„ç¤ºä¾‹

**æ–‡ç”Ÿå›¾ (Text-to-Image)**:

```python
from vllm_omni.entrypoints.omni import Omni

if __name__ == "__main__":
    # åˆ›å»º Omni å®ä¾‹
    omni = Omni(model="Tongyi-MAI/Z-Image-Turbo")
    
    # ç”Ÿæˆå›¾ç‰‡
    prompt = "a cup of coffee on the table"
    outputs = omni.generate(prompt)
    
    # ä¿å­˜ç»“æœ
    images = outputs[0].request_output[0].images
    images[0].save("coffee.png")
    print("Image saved to coffee.png")
```

**å¤šæ¨¡æ€å¯¹è¯ (Qwen3-Omni)**:

```python
from vllm import SamplingParams
from vllm_omni.entrypoints.omni import Omni

omni = Omni(model="Qwen/Qwen3-Omni-30B-A3B-Instruct")

# å‡†å¤‡è¾“å…¥
prompt = """<|im_start|>system
You are Qwen, a helpful assistant.<|im_end|>
<|im_start|>user
Hello, who are you?<|im_end|>
<|im_start|>assistant
"""

inputs = {"prompt": prompt}

# é‡‡æ ·å‚æ•°
sampling_params = [
    SamplingParams(temperature=0.9, max_tokens=512),  # Thinker
    SamplingParams(temperature=0.9, max_tokens=4096), # Talker
    SamplingParams(temperature=0.0, max_tokens=65536), # Code2wav
]

outputs = omni.generate(inputs, sampling_params)
```

#### 3. æ¢ç´¢ç¤ºä¾‹ç›®å½•

```
examples/offline_inference/
â”œâ”€â”€ text_to_image/      # æ–‡ç”Ÿå›¾ (å…¥é—¨æ¨è)
â”œâ”€â”€ text_to_video/      # æ–‡ç”Ÿè§†é¢‘
â”œâ”€â”€ text_to_audio/      # æ–‡ç”ŸéŸ³é¢‘
â”œâ”€â”€ image_to_image/     # å›¾åƒç¼–è¾‘
â”œâ”€â”€ image_to_video/     # å›¾ç”Ÿè§†é¢‘
â”œâ”€â”€ qwen2_5_omni/       # Qwen2.5-Omni
â”œâ”€â”€ qwen3_omni/         # Qwen3-Omni (æ¨è)
â”œâ”€â”€ qwen3_tts/          # Qwen3 TTS
â”œâ”€â”€ bagel/              # BAGEL
â””â”€â”€ lora_inference/     # LoRA æ¨ç†
```

---

### ğŸ“– é˜¶æ®µä¸‰ï¼šä»£ç é˜…è¯»ï¼ˆ2-3 å‘¨ï¼‰

**å­¦ä¹ ç›®æ ‡**ï¼šç†è§£æ ¸å¿ƒæ¶æ„

#### æ¨èé˜…è¯»é¡ºåº

| ä¼˜å…ˆçº§ | æ–‡ä»¶/æ¨¡å— | ç›®çš„ |
|--------|-----------|------|
| â­â­â­ | `vllm_omni/__init__.py` | ç†è§£å¯¼å‡ºçš„æ ¸å¿ƒç±» |
| â­â­â­ | `vllm_omni/entrypoints/omni.py` | ç†è§£ä¸»å…¥å£å’Œç®¡é“ç¼–æ’ |
| â­â­â­ | `docs/design/architecture_overview.md` | æ¶æ„è®¾è®¡æ–‡æ¡£ |
| â­â­ | `docs/design/module/ar_module.md` | AR æ¨¡å—è®¾è®¡ |
| â­â­ | `docs/design/module/dit_module.md` | Diffusion æ¨¡å—è®¾è®¡ |
| â­â­ | `vllm_omni/diffusion/diffusion_engine.py` | Diffusion å¼•æ“ |
| â­â­ | `vllm_omni/diffusion/scheduler.py` | Diffusion è°ƒåº¦å™¨ |
| â­ | `vllm_omni/config/model.py` | é…ç½®ç±»å®šä¹‰ |
| â­ | `vllm_omni/distributed/omni_connectors/` | åˆ†å¸ƒå¼è¿æ¥å™¨ |
| â­ | `vllm_omni/outputs.py` | è¾“å‡ºæ•°æ®ç»“æ„ |

#### å…³é”®ä»£ç è·¯å¾„

```
è¯·æ±‚è¿›æ¥
    â”‚
    â–¼
Omni.__init__()
    â”‚ åŠ è½½æ¨¡å‹é…ç½®
    â”‚ åˆ›å»º OmniStage åˆ—è¡¨
    â”‚ å¯åŠ¨ Worker è¿›ç¨‹
    â–¼
Omni.generate()
    â”‚ éªŒè¯è¾“å…¥å’Œé‡‡æ ·å‚æ•°
    â”‚ ç”Ÿæˆ request_id
    â–¼
_run_generation()
    â”‚ å°†è¯·æ±‚æ”¾å…¥ stage-0 é˜Ÿåˆ—
    â–¼
while completed < total:
    â”‚ è½®è¯¢å„é˜¶æ®µè¾“å‡ºé˜Ÿåˆ—
    â”‚
    â”œâ”€â”€ stage.try_collect()
    â”‚       â”‚ è·å–è¯¥é˜¶æ®µçš„è¾“å‡º
    â”‚       â–¼
    â”‚   if final_output:
    â”‚       yield OmniRequestOutput
    â”‚
    â””â”€â”€ è½¬å‘åˆ°ä¸‹ä¸€é˜¶æ®µ
            â”‚ process_engine_inputs()
            â”‚ connector.put() / stage.submit()
            â–¼
        ä¸‹ä¸€é˜¶æ®µå¤„ç†
```

#### è°ƒè¯•æŠ€å·§

```python
# å¯ç”¨è¯¦ç»†æ—¥å¿—
import logging
logging.basicConfig(level=logging.DEBUG)

# æˆ–è€…è®¾ç½®ç¯å¢ƒå˜é‡
import os
os.environ["VLLM_LOGGING_LEVEL"] = "DEBUG"
```

---

### ğŸ“– é˜¶æ®µå››ï¼šæ·±å…¥æ¨¡å—ï¼ˆ2-4 å‘¨ï¼‰

**å­¦ä¹ ç›®æ ‡**ï¼šæŒæ¡å…³é”®ç»„ä»¶å®ç°

#### 1. Diffusion æ¨¡å—æ·±å…¥

**å­¦ä¹ é¡ºåº**ï¼š

```
1. diffusion/data.py              # æ•°æ®ç»“æ„å®šä¹‰
   - OmniDiffusionConfig
   - OmniDiffusionRequest
   - DiffusionParallelConfig

2. diffusion/scheduler.py         # è°ƒåº¦å™¨å®ç°
   - Scheduler å•ä¾‹æ¨¡å¼
   - MessageQueue é€šä¿¡

3. diffusion/worker/              # Worker å®ç°
   - diffusion_worker.py          # è¿›ç¨‹å…¥å£
   - diffusion_model_runner.py    # æ¨¡å‹æ‰§è¡Œ

4. diffusion/models/qwen_image/   # å…·ä½“æ¨¡å‹å®ç°
   - pipeline.py                  # ç®¡é“å®šä¹‰
   - transformer.py               # Transformer å®ç°

5. diffusion/attention/           # æ³¨æ„åŠ›æœºåˆ¶
   - backends/                    # å„ç§åç«¯
   - parallel/                    # å¹¶è¡Œç­–ç•¥

6. diffusion/cache/               # ç¼“å­˜åŠ é€Ÿ
   - teacache/                    # TeaCache
   - cache_dit_backend.py         # cache-dit
```

#### 2. ç†è§£å¤šé˜¶æ®µç®¡é“

æŸ¥çœ‹ `model_executor/stage_configs/` ä¸‹çš„ YAML é…ç½®ï¼š

```bash
ls vllm_omni/model_executor/stage_configs/
# qwen2_5_omni.yaml
# qwen3_omni.yaml
# bagel.yaml
# ...
```

**ç†è§£æ•°æ®æµ**ï¼š

```
Qwen3-Omni ä¸‰é˜¶æ®µæµç¨‹:

ç”¨æˆ·è¾“å…¥ (æ–‡æœ¬/å›¾åƒ/è§†é¢‘/éŸ³é¢‘)
         â”‚
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Thinker â”‚  Stage 0 (AR)
    â”‚  æ€è€ƒå™¨  â”‚  è¾“å‡º: æ–‡æœ¬ tokens + hidden states
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚ OmniConnector
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Talker  â”‚  Stage 1 (AR)
    â”‚  è¯´è¯å™¨  â”‚  è¾“å‡º: éŸ³é¢‘ codec tokens
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚ OmniConnector
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚Code2wav â”‚  Stage 2 (Generation)
    â”‚ æ³¢å½¢ç”Ÿæˆ â”‚  è¾“å‡º: éŸ³é¢‘æ³¢å½¢
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    éŸ³é¢‘æ–‡ä»¶è¾“å‡º
```

#### 3. åˆ†å¸ƒå¼ç³»ç»Ÿ

å­¦ä¹  `distributed/omni_connectors/` çš„å®ç°ï¼š

```python
# å…±äº«å†…å­˜è¿æ¥å™¨
vllm_omni/distributed/omni_connectors/
â”œâ”€â”€ base.py                  # åŸºç±» OmniConnectorBase
â”œâ”€â”€ shared_memory.py         # SharedMemoryConnector
â”œâ”€â”€ mooncake.py              # MooncakeConnector
â”œâ”€â”€ yuanrong.py              # YuanrongConnector
â”œâ”€â”€ adapter.py               # é€‚é…å™¨
â””â”€â”€ utils/                   # å·¥å…·å‡½æ•°
```

---

### ğŸ“– é˜¶æ®µäº”ï¼šå®è·µé¡¹ç›®ï¼ˆæŒç»­ï¼‰

**å­¦ä¹ ç›®æ ‡**ï¼šèƒ½å¤Ÿè´¡çŒ®ä»£ç 

#### 1. å°è¯•ä¿®æ”¹ç¤ºä¾‹

```python
# ä¿®æ”¹é‡‡æ ·å‚æ•°
from vllm_omni.inputs.data import OmniDiffusionSamplingParams

sampling_params = OmniDiffusionSamplingParams(
    height=1024,
    width=1024,
    num_inference_steps=30,  # å‡å°‘æ­¥æ•°
    guidance_scale=7.5,      # è°ƒæ•´å¼•å¯¼å¼ºåº¦
    seed=42,
)
```

#### 2. é˜…è¯»æµ‹è¯•ç”¨ä¾‹

```bash
tests/
â”œâ”€â”€ e2e/                # ç«¯åˆ°ç«¯æµ‹è¯• (æ¨è)
â”‚   â”œâ”€â”€ test_omni_diffusion.py
â”‚   â””â”€â”€ test_omni_ar.py
â”œâ”€â”€ diffusion/          # Diffusion æ¨¡å—æµ‹è¯•
â”‚   â”œâ”€â”€ test_attention.py
â”‚   â””â”€â”€ test_cache.py
â”œâ”€â”€ distributed/        # åˆ†å¸ƒå¼æµ‹è¯•
â””â”€â”€ entrypoints/        # å…¥å£ç‚¹æµ‹è¯•
```

#### 3. å°è¯•æ·»åŠ æ–°æ¨¡å‹

å‚è€ƒç°æœ‰æ¨¡å‹å®ç°ï¼š

```bash
vllm_omni/diffusion/models/
â”œâ”€â”€ interface.py         # æ¨¡å‹æ¥å£ (å¿…è¯»)
â”œâ”€â”€ qwen_image/          # Qwen-Image (å‚è€ƒ)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ pipeline.py      # ç®¡é“å®ç°
â”‚   â””â”€â”€ transformer.py   # æ¨¡å‹å®ç°
â””â”€â”€ flux/                # FLUX (å¦ä¸€ä¸ªå‚è€ƒ)
```

é˜…è¯»å®˜æ–¹æ–‡æ¡£: `docs/contributing/model/adding_diffusion_model.md`

---

## å­¦ä¹ æŠ€å·§

### 1. å–„ç”¨è®¾è®¡æ–‡æ¡£

```
docs/design/
â”œâ”€â”€ architecture_overview.md    # å¿…è¯»ï¼æ•´ä½“æ¶æ„
â”œâ”€â”€ feature/
â”‚   â”œâ”€â”€ disaggregated_inference.md  # åˆ†ç¦»å¼æ¨ç†
â”‚   â””â”€â”€ ray_based_execution.md      # Ray æ‰§è¡Œ
â””â”€â”€ module/
    â”œâ”€â”€ ar_module.md            # AR æ¨¡å—è¯¦è§£
    â””â”€â”€ dit_module.md           # Diffusion è¯¦è§£
```

### 2. æ–­ç‚¹è°ƒè¯•

åœ¨å…³é”®ä½ç½®è®¾ç½®æ–­ç‚¹ï¼š

```python
# vllm_omni/entrypoints/omni.py
def generate(self, prompts, ...):
    # åœ¨è¿™é‡Œè®¾ç½®æ–­ç‚¹
    ...

# vllm_omni/diffusion/diffusion_engine.py
def step(self, requests):
    # åœ¨è¿™é‡Œè®¾ç½®æ–­ç‚¹
    ...
```

### 3. æ—¥å¿—è¾“å‡º

```python
# æ–¹å¼ 1: Python logging
import logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# æ–¹å¼ 2: ç¯å¢ƒå˜é‡
import os
os.environ["VLLM_LOGGING_LEVEL"] = "DEBUG"
os.environ["VLLM_TRACE_FUNCTION"] = "1"  # å‡½æ•°è°ƒç”¨è¿½è¸ª
```

### 4. å…³æ³¨æ ¸å¿ƒæ¦‚å¿µ

| æ¦‚å¿µ | æè¿° | ä½ç½® |
|------|------|------|
| `OmniStage` | é˜¶æ®µæŠ½è±¡ | `entrypoints/omni_stage.py` |
| `OmniRequestOutput` | è¾“å‡ºç»“æ„ | `outputs.py` |
| `OmniConnector` | é˜¶æ®µé—´é€šä¿¡ | `distributed/omni_connectors/` |
| `SamplingParams` | AR é‡‡æ ·å‚æ•° | vLLM |
| `OmniDiffusionSamplingParams` | Diffusion é‡‡æ ·å‚æ•° | `inputs/data.py` |
| `OmniModelConfig` | æ¨¡å‹é…ç½® | `config/model.py` |

### 5. ä½¿ç”¨ IDE åŠŸèƒ½

- **Go to Definition**: è·³è½¬åˆ°å®šä¹‰
- **Find References**: æŸ¥æ‰¾å¼•ç”¨
- **Call Hierarchy**: è°ƒç”¨å±‚æ¬¡
- **Type Hierarchy**: ç±»å‹å±‚æ¬¡

---

## é¡¹ç›®äº®ç‚¹æ€»ç»“

| ç‰¹æ€§ | æè¿° |
|------|------|
| ğŸ”¥ **å¤šæ¨¡æ€æ”¯æŒ** | æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘ã€éŸ³é¢‘è¾“å…¥è¾“å‡º |
| ğŸš€ **é«˜æ€§èƒ½** | ç»§æ‰¿ vLLM çš„ KV Cache ä¼˜åŒ– + Diffusion åŠ é€Ÿ |
| ğŸ”§ **çµæ´»ç®¡é“** | å¯é…ç½®çš„å¤šé˜¶æ®µå¼‚æ„ç®¡é“ |
| ğŸŒ **åˆ†å¸ƒå¼** | æ”¯æŒå¤šèŠ‚ç‚¹åˆ†å¸ƒå¼æ¨ç† |
| ğŸ”Œ **æ˜“æ‰©å±•** | æ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºæ·»åŠ æ–°æ¨¡å‹ |
| ğŸ“Š **å¤šå¹³å°** | æ”¯æŒ CUDAã€ROCmã€NPUã€XPU |
| ğŸ¯ **OpenAI å…¼å®¹** | æä¾› OpenAI å…¼å®¹çš„ API æœåŠ¡å™¨ |

---

## å¸¸è§é—®é¢˜ FAQ

### Q1: å¦‚ä½•é€‰æ‹©åˆé€‚çš„æ¨¡å‹ï¼Ÿ

| ä»»åŠ¡ | æ¨èæ¨¡å‹ |
|------|----------|
| æ–‡ç”Ÿå›¾ | Z-Image-Turbo, Qwen-Image, FLUX |
| æ–‡ç”Ÿè§†é¢‘ | Wan2.2 |
| å¤šæ¨¡æ€å¯¹è¯ | Qwen3-Omni, Qwen2.5-Omni |
| å›¾åƒç†è§£+ç”Ÿæˆ | BAGEL |

### Q2: å†…å­˜ä¸å¤Ÿæ€ä¹ˆåŠï¼Ÿ

```python
# å¯ç”¨ CPU å¸è½½
omni = Omni(
    model="...",
    enable_cpu_offload=True,
)

# æˆ–è€…ä½¿ç”¨åˆ†å±‚å¸è½½
omni = Omni(
    model="...",
    enable_layerwise_offload=True,
    layerwise_num_gpu_layers=1,
)

# VAE ä¼˜åŒ–
omni = Omni(
    model="...",
    vae_use_slicing=True,
    vae_use_tiling=True,
)
```

### Q3: å¦‚ä½•ä½¿ç”¨å¤š GPUï¼Ÿ

```python
from vllm_omni.diffusion.data import DiffusionParallelConfig

parallel_config = DiffusionParallelConfig(
    tensor_parallel_size=2,      # å¼ é‡å¹¶è¡Œ
    ulysses_degree=2,            # Ulysses åºåˆ—å¹¶è¡Œ
    ring_degree=1,               # Ring åºåˆ—å¹¶è¡Œ
    cfg_parallel_size=2,         # CFG å¹¶è¡Œ
)

omni = Omni(
    model="...",
    parallel_config=parallel_config,
)
```

### Q4: å¦‚ä½•ä½¿ç”¨ç¼“å­˜åŠ é€Ÿï¼Ÿ

```python
# TeaCache
omni = Omni(
    model="...",
    cache_backend="tea_cache",
    cache_config={"rel_l1_thresh": 0.2},
)

# cache-dit
omni = Omni(
    model="...",
    cache_backend="cache_dit",
    cache_config={
        "Fn_compute_blocks": 1,
        "max_warmup_steps": 4,
        "residual_diff_threshold": 0.24,
    },
)
```

---

## å‚è€ƒèµ„æº

### å®˜æ–¹èµ„æº

- [vLLM-Omni GitHub](https://github.com/vllm-project/vllm-omni)
- [vLLM-Omni æ–‡æ¡£](https://vllm-omni.readthedocs.io/)
- [vLLM å®˜æ–¹æ–‡æ¡£](https://docs.vllm.ai/)

### è®ºæ–‡

- [vLLM-Omni Paper](https://arxiv.org/abs/2602.02204)
- [vLLM Paper](https://arxiv.org/abs/2309.06180)

### ç¤¾åŒº

- Slack: `#sig-omni` @ [slack.vllm.ai](https://slack.vllm.ai)
- è®ºå›: [discuss.vllm.ai](https://discuss.vllm.ai)

---

*æœ¬æ–‡æ¡£ç”± AI åŠ©æ‰‹ç”Ÿæˆï¼Œæœ€åæ›´æ–°: 2026-02-03*

